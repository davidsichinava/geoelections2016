## Load all necessary libraries
setwd("D:\\Dropbox\\My Projects\\Politicians_Data\\Rep\\geo_poltc_data")
library(xlsx)
library(stringi)
library(stringr)
library(plyr)
library(dplyr)
library(ggmap)
library(stringdist)
library(tidyr)
library(qdap)
## require(nominatim) ## requires devtools. run this: devtools::install_github("hrbrmstr/nominatim")
## require(devtools)
## Source necessary custom functions
source("src/func.R")
source("src/clean/cln2012.R")
a<-unlist(a)
c<-paste0(as.character(unique(a)), ", Georgia (country)")
e<-geocode(c, source="google", output="more")
library(XML)
library(rvest)
library(plyr)
library(dplyr)
library(httr)
library(zoo)
library(gdata)
library(qpcR)
library(data.table)
library(stringr)
library(foreach)
setwd("D:\\Dropbox\\My Projects\\Elections\\Geogia 2016 Parliamentary\\github\\geoelections2016")
install.packages("qpcR")
install.packages("rvest")
library(XML)
library(rvest)
library(plyr)
library(dplyr)
library(httr)
library(zoo)
library(gdata)
library(qpcR)
library(data.table)
library(stringr)
library(foreach)
setwd("D:\\Dropbox\\My Projects\\Elections\\Geogia 2016 Parliamentary\\github\\geoelections2016")
## Load necessary libraries
library(XML)
library(rvest)
library(plyr)
library(dplyr)
library(httr)
library(zoo)
library(gdata)
library(qpcR)
library(data.table)
library(stringr)
library(foreach)
setwd("D:\\Dropbox\\My Projects\\Elections\\Geogia 2016 Parliamentary\\github\\geoelections2016")
## Scrape precinct-level Proportional results
## Get URLs for each majoritarian district
mURL<-"http://results.cec.gov.ge/proporciuli.html" # Assign master page url
url <- html_session(mURL) # Get master page
urls <- url %>% # feed `main.page` to the next step
html_nodes(". a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
urls <- url %>% # feed `main.page` to the next step
html_nodes(".a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
urls <- url %>% # feed `main.page` to the next step
html_nodes("a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
# Save as data frame
dfURL <- data.frame(urls = urls, stringsAsFactors = FALSE)
View(dfURL)
scrURL<-html_session(paste0("http://results.cec.gov.ge/", ("olq_46.html", sep=""))
scrURL<-html_session(paste0("http://results.cec.gov.ge/", ("olq_46.html"), sep=""))
t1Data<-scrURL%>%
html_node(".td")%>%
html_table(fill=TRUE, header=TRUE)
dataTbl<-scrURL%>%
read_html() %>%
html_nodes(xpath='//*[(@id = "table36")]//td')%>%
html_table(fill=TRUE, header=TRUE)
dataTbl<-scrURL%>%
read_html() %>%
html_nodes(xpath='//*[(@id = "table36")]')%>%
html_table(fill=TRUE, header=TRUE)
dataTbl<-as.data.frame(dataTbl)
View(dataTbl)
dfURL<-dfURL[grep("*olq*", rownames(dfURL)), ]
mURL<-"http://results.cec.gov.ge/proporciuli.html" # Assign master page url
url <- html_session(mURL) # Get master page
urls <- url %>% # feed `main.page` to the next step
html_nodes("a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
dfURL <- data.frame(urls = urls, stringsAsFactors = FALSE)
View(dfURL)
a<-dfURL[grep("*olq*", dfURL$urls), ]
a<-as.data.frame(dfURL[grep("*olq*", dfURL$urls), ])
View(a)
dfURL<-as.data.frame(dfURL[grep("*olq*", dfURL$urls), ])
names(dfURL)<-c("urls")
View(dfURL)
## Loop thourgh URLs and get data tables for each majoritarian district
PropData <- (setNames(replicate(7,numeric(0), simplify = F), letters[1:27]))
PropData <- (setNames(replicate(27,numeric(0), simplify = F), letters[1:27]))
PropData <- as.data.frame(setNames(replicate(27,numeric(0), simplify = F), letters[1:27]))
View(PropData)
names(PropData)<-c("PrecinctID", "Party_1", "Party_2", "Party_3", "Party_4", "Party_5", "Party_6", "Party_7", "Party_8", "Party_10", "Party_11", "Party_12", "Party_14", "Party_15", "Party_16", "Party_17", "Party_18", "Party_19", "Party_22", "Party_23", "Party_25", "Party_26", "Party_27", "Party_28", "Party_30", "Party_41", "Protocol")
for(i in 1:nrow(dfURL)) {
scrURL<-html_session(paste0("http://results.cec.gov.ge/", (dfURL$urls[i]), sep=""))
propRaw<-scrURL%>%
read_html() %>%
html_nodes(xpath='//*[(@id = "table36")]')%>%
html_table(fill=TRUE, header=TRUE)
propRaw<-as.data.frame(propRaw)
PropData<-rbind(PropData, propRaw)
}
View(PropData)
View(propRaw)
PropData <- as.data.frame(setNames(replicate(27,numeric(0), simplify = F), letters[1:27]))
for(i in 1:nrow(dfURL)) {
scrURL<-html_session(paste0("http://results.cec.gov.ge/", (dfURL$urls[i]), sep=""))
propRaw<-scrURL%>%
read_html() %>%
html_nodes(xpath='//*[(@id = "table36")]')%>%
html_table(fill=TRUE, header=TRUE)
propRaw<-as.data.frame(propRaw)
PropData<-rbind(PropData, propRaw)
}
View(propRaw)
View(PropData)
PropData <- as.data.frame(setNames(replicate(27,numeric(0), simplify = F), letters[1:27]))
for(i in 1:nrow(dfURL)) {
scrURL<-html_session(paste0("http://results.cec.gov.ge/", (dfURL$urls[i]), sep=""))
propRaw<-scrURL%>%
read_html(encoding = "UTF-8") %>%
html_nodes(xpath='//*[(@id = "table36")]')%>%
html_table(fill=TRUE, header=TRUE)
propRaw<-as.data.frame(propRaw)
PropData<-rbind(PropData, propRaw)
Sys.sleep(1)
}
View(PropData)
PropData <- as.data.frame(setNames(replicate(27,numeric(0), simplify = F), letters[1:27]))
for(i in 1:nrow(dfURL)) {
scrURL<-html_session(paste0("http://results.cec.gov.ge/", (dfURL$urls[i]), sep=""))
propRaw<-scrURL%>%
read_html(encoding = "UTF-8") %>%
html_nodes(xpath='//*[(@id = "table36")]')%>%
html_table(fill=TRUE, header=TRUE)
propRaw<-as.data.frame(propRaw)
PropData<-rbind(PropData, propRaw)
Sys.sleep(5)
}
View(PropData)
dfURL<-(dfURL[grep("*olq*", dfURL$urls), ])
dfURL<-paste0("http://results.cec.gov.ge/", (dfURL$urls[i]), sep="")
dfURL<-paste0("http://results.cec.gov.ge/", dfUR, sep="")
dfURL<-paste0("http://results.cec.gov.ge/", dfURL, sep="")
lapply(seq_along(dfURL),
function(x) write.table(balstables[[x]], paste0("dfURL", tab.nums[x], ".txt"), row.name=FALSE))
lapply(seq_along(dfURL),
function(x) write.table(dfURL[[x]], paste0("dfURL", tab.nums[x], ".txt"), row.name=FALSE))
names(dfURL)<-c("urls")
dfURL<-as.data.frame(dfURL[grep("*olq*", dfURL$urls), ])
mURL<-"http://results.cec.gov.ge/proporciuli.html" # Assign master page url
url <- html_session(mURL) # Get master page
urls <- url %>% # feed `main.page` to the next step
html_nodes("a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
dfURL <- data.frame(urls = urls, stringsAsFactors = FALSE)
## Subset those districts which have data
dfURL<-as.data.frame(dfURL[grep("*olq*", dfURL$urls), ])
names(dfURL)<-c("urls")
View(dfURL)
dfURL<-(dfURL[grep("*olq*", dfURL$urls), ])
dfURL<-paste0("http://results.cec.gov.ge/", dfURL, sep="")
PropData <- as.data.frame(setNames(replicate(27,numeric(0), simplify = F), letters[1:27]))
for(i in 1:nrow(dfURL)) {
scrURL<-html_session(paste0("http://results.cec.gov.ge/", (dfURL$urls[i]), sep=""))
propRaw<-scrURL%>%
read_html(handle = curl::new_handle("useragent" = "Mozilla/5.0"), encoding = "UTF-8") %>%
html_nodes(xpath='//*[(@id = "table36")]')%>%
html_table(fill=TRUE, header=TRUE)
propRaw<-as.data.frame(propRaw)
PropData<-rbind(PropData, propRaw)
Sys.sleep(5)
}
dfURL<-as.data.frame(paste0("http://results.cec.gov.ge/", dfURL, sep=""))
for(i in 1:nrow(dfURL)) {
scrURL<-html_session(paste0("http://results.cec.gov.ge/", (dfURL$urls[i]), sep=""))
propRaw<-scrURL%>%
read_html(handle = curl::new_handle("useragent" = "Mozilla/5.0"), encoding = "UTF-8") %>%
html_nodes(xpath='//*[(@id = "table36")]')%>%
html_table(fill=TRUE, header=TRUE)
propRaw<-as.data.frame(propRaw)
PropData<-rbind(PropData, propRaw)
Sys.sleep(5)
}
View(PropData)
View(dataTbl)
